
# Secure AI Systems â€“ Red & Blue Teaming on MNIST (IDX Format)

This repository contains the complete implementation, experiments, visualizations, and security analysis for a **Convolutional Neural Network (CNN)** trained on the MNIST handwritten digit dataset (IDX format). 
The project demonstrates **real-world adversarial machine learning attacks and defenses**, including:

- âœ”ï¸ Data poisoning (backdoor attack)
- âœ”ï¸ FGSM adversarial attack (evasion attack)
- âœ”ï¸ Adversarial training (defense mechanism)
- âœ”ï¸ STRIDE threat modeling
- âœ”ï¸ Static Application Security Testing (SAST) using Bandit
- âœ”ï¸ Full LaTeX report + Plots + PPT summary

This repository fully satisfies the assignment deliverables for: 
**â€œSecure AI Systems â€“ Red and Blue Teaming an MNIST Classifier.â€**

---

# Repository Structure

```
mnist-secure-ai/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ mnist_secure_cnn_idx.py
â”‚
â”‚â”œâ”€â”€ data/
â”‚   â””â”€â”€ MNIST
â”œâ”€â”€ report/
â”‚   â”œâ”€â”€ mnist_secure_ai_report.pdf
â”‚   â”œâ”€â”€ bandit_report.json
â”‚   â””â”€â”€ bandit_report_Fix.json
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

âš ï¸ **Note:** MNIST IDX dataset files are intentionally excluded via `.gitignore`.

---

# ğŸš€ How to Run This Project

### 1ï¸âƒ£ Create a virtual environment
```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2ï¸âƒ£ Place MNIST IDX files in:
```
mnist-secure-ai/data/
```

Required files:
- train-images.idx3-ubyte
- train-labels.idx1-ubyte
- t10k-images.idx3-ubyte
- t10k-labels.idx1-ubyte

### 3ï¸âƒ£ Run Baseline Training
```bash
python src/mnist_secure_cnn_idx.py --mode baseline --epochs 5
```

### 4ï¸âƒ£ Run Data Poisoning Attack
```bash
python src/mnist_secure_cnn_idx.py --mode poisoned --epochs 5 --poison_count 100 --poison_target_label 1
```

### 5ï¸âƒ£ Generate FGSM Adversarial Samples
```bash
python src/mnist_secure_cnn_idx.py --mode adv_only --epochs 5 --epsilon_fgsm 0.3
```

### 6ï¸âƒ£ Perform Adversarial Training Defense
```bash
python src/mnist_secure_cnn_idx.py --mode adv_training --epochs 5 --epsilon_fgsm 0.3
```

---

# ğŸ“Š Performance Summary

## ğŸ”µ Baseline Model
- **Accuracy:** 98.91%
- **Loss:** 0.0400  
- **Inference Time:** ~5.94 m

---

## ğŸ”´ Data Poisoning Attack (Backdoor)
- 100 poisoned samples (7 â†’ 1)
- White square trigger added
- **Clean Test Accuracy:** 98.69% (backdoor remains hidden)
- Strong misclassification when trigger is present


---

## âš¡ FGSM Adversarial Attack (Îµ = 0.3)
- **Clean Accuracy:** 99.07%
- **FGSM Accuracy:** **17.5%** â†’ severe degradation

---

## ğŸ›¡ Adversarial Training Defense

**Before defense**
- FGSM accuracy: **17.5%**

**After defense**
- FGSM accuracy: **81.56%**
- Clean accuracy preserved: **98.96%**


---

# ğŸ›¡ STRIDE Threat Model (Summary)

| STRIDE | Threat | Mitigation in This Project |
|--------|--------|----------------------------|
| **S â€“ Spoofing** | Fake data sources | Controlled IDX loading, no remote inputs |
| **T â€“ Tampering** | Poisoned data | Explicit poisoning module demonstrates risk; integrity checks prevent accidental tampering |
| **R â€“ Repudiation** | Hidden malicious actions | All runs require explicit modes (baseline/poisoned/adv_only/adv_training) |
| **I â€“ Information Disclosure** | Leaking model/data | All operations are offline/local |
| **D â€“ DoS** | Adversarial input overload | Inference-time benchmarking under attack performed |
| **E â€“ Elevation of Privilege** | Unauthorized retraining | Clear separation of training modes and poisoning logic |

Full STRIDE analysis is available in the LaTeX report.

---

# ğŸ” Static Application Security Testing (SAST)

Tool Used: **Bandit**

### Results:
- Low-severity: Use of `assert` (fixed to `ValueError`)
- No medium/high issues in our implementation
- Reports included:
  - bandit_report.json
  - bandit_report_Fix.json

---

# ğŸ Conclusion

This project demonstrates that:

- High accuracy **does not guarantee** robustness.
- A small poisoning set (100 samples) can implant a stealthy backdoor.
- FGSM adversarial noise can **break** the classifier completely.
- Adversarial training improves robustness from **17.5% â†’ 81.56%**.
- STRIDE analysis and SAST enhance the overall security posture.

---

# âœï¸ Author

- Name : Afzaal Ahmad
- Department of Computer Science  
- Indian Institute of Technology Hyderabad
